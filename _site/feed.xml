<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2024-10-30T06:36:14+01:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Yu Zhao</title><subtitle>Yu Zhao&apos;s personal website</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;&quot;/assets/images/yuzhao-avatar.jpg&quot;, &quot;bio&quot;=&gt;&quot;Yu Zhao&quot;, &quot;location&quot;=&gt;&quot;Edinburgh, Scotland, UK&quot;, &quot;email&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-square-x-twitter&quot;, &quot;url&quot;=&gt;&quot;https://x.com/yuzhaouoe&quot;}, {&quot;label&quot;=&gt;&quot;Email&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-envelope-square&quot;, &quot;url&quot;=&gt;&quot;mailto:yuzhaouoe@gmail.com&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/yuzhaouoe&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/yu-zhao-b303482b3/&quot;}, {&quot;label&quot;=&gt;&quot;Semantic Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-solid fa-magnifying-glass&quot;, &quot;url&quot;=&gt;&quot;https://www.semanticscholar.org/author/Yu-Zhao/2155474139&quot;}, {&quot;label&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google-scholar&quot;, &quot;url&quot;=&gt;&quot;https://scholar.google.com/citations?user=QR0LL6gAAAAJ&quot;}]}</name></author><entry><title type="html">LogitLens from scratch with Hugging Face Transformers</title><link href="http://0.0.0.0:4000/LogitLens/" rel="alternate" type="text/html" title="LogitLens from scratch with Hugging Face Transformers" /><published>2024-10-28T00:00:00+01:00</published><updated>2024-10-28T00:00:00+01:00</updated><id>http://0.0.0.0:4000/LogitLens</id><content type="html" xml:base="http://0.0.0.0:4000/LogitLens/">&lt;p&gt;In this short tutorial, we’ll implement LogitLens to inspect the inner representations of a pre-trained &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Phi-1.5&lt;/code&gt;. &lt;a href=&quot;https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens&quot;&gt;LogitLens&lt;/a&gt; is a straightforward yet effective interpretability method.&lt;/p&gt;

&lt;p&gt;The core idea behind it is to apply the language model’s output layer (also known as the “unembedding matrix” or “language modeling head”) to the hidden states at each layer of the transformer. This allows us to see how the model’s internal representations change as the input progresses through the network. Surprisingly, the model often acquires a significant amount of semantic understanding in the earlier layers of the transformer. By inspecting the predicted tokens at each layer, we can observe how the model’s understanding of the input evolves.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: ✋ If you’re looking for advanced interpretability tools, there are plenty of powerful libraries out there. But here, we’re going back to basics and do this from scratch because it’s always cool to understand how things work under the hood.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can also &lt;a href=&quot;https://drive.google.com/file/d/1nTGbjz4AK7QZqq5BgzQozqHcjpIAndCG&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’ll use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Microsoft Phi-1.5 &lt;/code&gt;here since it’s a small, open model. Feel free to swap in another Hugging Face model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;microsoft/phi-1.5&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# load the model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_bos_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bos_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&amp;lt;bos&amp;gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_fast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Downloading the model might take a while, so you better pick a small model :).
Let’s now consider an example input sentence and tokenize it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The quick brown fox jumps over the lazy&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Input shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input shape:  torch.Size([1, 9])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The sentence was encoded into 9 tokens. In case you want to know what the tokens looks like, you can just decode them back:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_ids_to_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Input tokens: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input tokens:  [&apos;&amp;lt;bos&amp;gt;&apos;, &apos;The&apos;, &apos;Ġquick&apos;, &apos;Ġbrown&apos;, &apos;Ġfox&apos;, &apos;Ġjumps&apos;, &apos;Ġover&apos;, &apos;Ġthe&apos;, &apos;Ġlazy&apos;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the tokenizer added the beggining of sentence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bos&amp;gt;&lt;/code&gt; token. The ugly &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ġ&lt;/code&gt; represent spaces.&lt;/p&gt;

&lt;p&gt;In the notebook you can find a function clean these up a bit (I find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ġ&lt;/code&gt;s are really annoying):&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cleanup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&apos;&amp;lt;bos&amp;gt;&apos;, &apos;The&apos;, &apos; quick&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s feed the input into the model to get the next token prediction along with all the hidden states. Fortunately, the model’s forward method provides an option to return its hidden states.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# we need all the intermediate hidden states
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_hidden_states&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# # print(outputs.keys())
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Logits shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logits&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Logits shape:  torch.Size([1, 9, 51200]) # (batch, sequence len, vocab size)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The logits have been already projected into the vocabulary space. Hidden states on the other hand are still “raw” token representations. We’ll have one hiddent state vector for each model layer.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Number of model layers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hidden states for first layer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Number of model layers:  25
Hidden states for first layer:  torch.Size([1, 9, 2048])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we see, each layer in the model produces a hidden state. Here the last dimension represents the embedding size (not the vocbulary size).&lt;/p&gt;

&lt;p&gt;By applying the language modeling head (or unembedding matrix) to the hidden state at any layer, we can generate ‘early’ logits—predictions from intermediate representations. While the model isn’t explicitly trained to produce meaningful logits at these layers, we’ll see that it naturally starts embedding token-level information along the way.&lt;/p&gt;

&lt;p&gt;We can apply the language modeling head like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;logits_at_second_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Logits at second layer shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits_at_second_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Logits at second layer shape:  torch.Size([1, 9, 51200])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We now want to access the hidden state at each layer, apply the language modeling head to get the logits, and finally decode the logits into tokens.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# apply the language model head to the hidden states
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# decode the logits to get the predicted token ids
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predicted_token_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# convert the token ids to tokens
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_ids_to_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_token_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cleanup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# append the predicted tokens to the list for later
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Layer &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Layer 0: [&apos;-&apos;, &apos; S&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos; S&apos;, &apos;-&apos;]
Layer 1: [&apos;ed&apos;, &apos;oret&apos;, &apos;est&apos;, &apos;ies&apos;, &apos;es&apos;, &apos;uit&apos;, &apos; the&apos;, &apos; same&apos;, &apos; double&apos;]
Layer 2: [&apos;import&apos;, &apos;oret&apos;, &apos;est&apos;, &apos;ies&apos;, &apos;es&apos;, &apos;uits&apos;, &apos; time&apos;, &apos; same&apos;, &apos; part&apos;]
Layer 3: [&apos;import&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;es&apos;, &apos; all&apos;, &apos; entire&apos;, &apos; man&apos;]
Layer 4: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;worked&apos;, &apos; entire&apos;, &apos; man&apos;]
Layer 5: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;worked&apos;, &apos; entire&apos;, &apos; man&apos;]
Layer 6: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;kill&apos;, &apos;ses&apos;, &apos; man&apos;]
Layer 7: [&apos;iveness&apos;, &apos;orem&apos;, &apos;est&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; all&apos;, &apos; entire&apos;, &apos; brown&apos;]
Layer 8: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos;ind&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 9: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 10: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; ph&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 11: [&apos;iveness&apos;, &apos;orem&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 12: [&apos;iveness&apos;, &apos;oret&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 13: [&apos;ality&apos;, &apos;oret&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 14: [&apos;ality&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; ph&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 15: [&apos;iveness&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 16: [&apos;import&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; lazy&apos;, &apos; entire&apos;, &apos; poor&apos;]
Layer 17: [&apos;import&apos;, &apos;mes&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; poor&apos;]
Layer 18: [&apos;import&apos;, &apos; first&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 19: [&apos; example&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;Ċ&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 20: [&apos;ĊĊ&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;s&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 21: [&apos;ing&apos;, &apos; first&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 22: [&apos;Ċ&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;Ċ&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 23: [&apos;Ċ&apos;, &apos;Ċ&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; J&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
Layer 24: [&apos;Ċ&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you observe, the predictions refine layer-by-layer, reflecting the model’s gradual understanding of the input.
We can visualize the predictions with a heatmap:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# create a heatmap that has a row for each list in the logitlens list
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_theme&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;white&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# just for the bkg color
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intensities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create heatmap
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intensities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;annot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cleanup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Greys&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;xticklabels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;yticklabels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invert_yaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/alessiodevoto/alessiodevoto.github.io/refs/heads/main/assets/images/logitlens/logit_small.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Right now, our heatmap just displays the model’s top predictions (using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argmax&lt;/code&gt;), which is fine but a bit flat. Let’s make it more interesting by incorporating model certainty into the visualization.&lt;/p&gt;

&lt;p&gt;A good way to quantify the model’s certainity about its output is looking at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)&quot;&gt;entropy&lt;/a&gt; of the output distribution. Let’s replace the background color of each cell with the entropy of the model when generating that token.&lt;/p&gt;

&lt;p&gt;We’ll calculate the entropy of the output distribution, using it to color the background:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# aux function to compute the entropy from logits
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;entropy_from_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#avoid nans
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can run the same code as before, this time we’ll also compute and store the entropies.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# apply the language model head to the hidden states
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# get the entropy of the logits
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy_from_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# decode the logits to get the predicted token ids
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predicted_token_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# convert the token ids to tokens
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_ids_to_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_token_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cleanup_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# append the predicted tokens to the list
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entropies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Layer &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_tokens&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s now create a plot where each cell is colored based on the entropy.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Create figure and axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create heatmap
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;annot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;YlGnBu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;xticklabels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_input_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;yticklabels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logitlens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invert_yaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/alessiodevoto/alessiodevoto.github.io/refs/heads/main/assets/images/logitlens/logitlens_small.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hope you liked this! If you have any suggestions/questios, feel free to drop me a message/email or visit &lt;a href=&quot;https://alessiodevoto.github.io/&quot;&gt;my page&lt;/a&gt; or my twitter &lt;a href=&quot;https://x.com/devoto_alessio&quot;&gt;@devoto_alessio&lt;/a&gt;.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;&quot;/assets/images/yuzhao-avatar.jpg&quot;, &quot;bio&quot;=&gt;&quot;Yu Zhao&quot;, &quot;location&quot;=&gt;&quot;Edinburgh, Scotland, UK&quot;, &quot;email&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-square-x-twitter&quot;, &quot;url&quot;=&gt;&quot;https://x.com/yuzhaouoe&quot;}, {&quot;label&quot;=&gt;&quot;Email&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-envelope-square&quot;, &quot;url&quot;=&gt;&quot;mailto:yuzhaouoe@gmail.com&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/yuzhaouoe&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/yu-zhao-b303482b3/&quot;}, {&quot;label&quot;=&gt;&quot;Semantic Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-solid fa-magnifying-glass&quot;, &quot;url&quot;=&gt;&quot;https://www.semanticscholar.org/author/Yu-Zhao/2155474139&quot;}, {&quot;label&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google-scholar&quot;, &quot;url&quot;=&gt;&quot;https://scholar.google.com/citations?user=QR0LL6gAAAAJ&quot;}]}</name></author><summary type="html">In this short tutorial, we’ll implement LogitLens to inspect the inner representations of a pre-trained Phi-1.5. LogitLens is a straightforward yet effective interpretability method. The core idea behind it is to apply the language model’s output layer (also known as the “unembedding matrix” or “language modeling head”) to the hidden states at each layer of the transformer. This allows us to see how the model’s internal representations change as the input progresses through the network. Surprisingly, the model often acquires a significant amount of semantic understanding in the earlier layers of the transformer. By inspecting the predicted tokens at each layer, we can observe how the model’s understanding of the input evolves. Disclaimer: ✋ If you’re looking for advanced interpretability tools, there are plenty of powerful libraries out there. But here, we’re going back to basics and do this from scratch because it’s always cool to understand how things work under the hood. You can also We’ll use Microsoft Phi-1.5 here since it’s a small, open model. Feel free to swap in another Hugging Face model. from transformers import AutoModelForCausalLM, AutoTokenizer import torch model_id= &quot;microsoft/phi-1.5&quot; # load the model model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).eval().to(device) tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, bos_token=&apos;&amp;lt;bos&amp;gt;&apos;, use_fast=False) Downloading the model might take a while, so you better pick a small model :). Let’s now consider an example input sentence and tokenize it. example = &quot;The quick brown fox jumps over the lazy&quot; inputs = tokenizer(example, return_tensors=&quot;pt&quot;).to(device) print(&quot;Input shape: &quot;, inputs[&quot;input_ids&quot;].shape) Input shape: torch.Size([1, 9]) The sentence was encoded into 9 tokens. In case you want to know what the tokens looks like, you can just decode them back: original_input_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0], skip_special_tokens=False) print(&quot;Input tokens: &quot;, original_input_tokens) Input tokens: [&apos;&amp;lt;bos&amp;gt;&apos;, &apos;The&apos;, &apos;Ġquick&apos;, &apos;Ġbrown&apos;, &apos;Ġfox&apos;, &apos;Ġjumps&apos;, &apos;Ġover&apos;, &apos;Ġthe&apos;, &apos;Ġlazy&apos;] As we can see, the tokenizer added the beggining of sentence &amp;lt;bos&amp;gt; token. The ugly Ġ represent spaces. In the notebook you can find a function clean these up a bit (I find the Ġs are really annoying): original_input_tokens = cleanup_tokens(original_input_tokens) original_input_tokens [&apos;&amp;lt;bos&amp;gt;&apos;, &apos;The&apos;, &apos; quick&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;] Now, let’s feed the input into the model to get the next token prediction along with all the hidden states. Fortunately, the model’s forward method provides an option to return its hidden states. # we need all the intermediate hidden states with torch.no_grad(): outputs = model(**inputs, output_hidden_states=True) # # print(outputs.keys()) print(&quot;Logits shape: &quot;, outputs[&quot;logits&quot;].shape) Logits shape: torch.Size([1, 9, 51200]) # (batch, sequence len, vocab size) The logits have been already projected into the vocabulary space. Hidden states on the other hand are still “raw” token representations. We’ll have one hiddent state vector for each model layer. hidden_states = outputs.hidden_states print(&quot;Number of model layers&quot;, len(hidden_states)) print(&quot;Hidden states for first layer&quot;, hidden_states[0].shape) Number of model layers: 25 Hidden states for first layer: torch.Size([1, 9, 2048]) As we see, each layer in the model produces a hidden state. Here the last dimension represents the embedding size (not the vocbulary size). By applying the language modeling head (or unembedding matrix) to the hidden state at any layer, we can generate ‘early’ logits—predictions from intermediate representations. While the model isn’t explicitly trained to produce meaningful logits at these layers, we’ll see that it naturally starts embedding token-level information along the way. We can apply the language modeling head like this: logits_at_second_layer = model.lm_head(hidden_states[2]) print(&quot;Logits at second layer shape: &quot;, logits_at_second_layer.shape) Logits at second layer shape: torch.Size([1, 9, 51200]) We now want to access the hidden state at each layer, apply the language modeling head to get the logits, and finally decode the logits into tokens. for i, hidden_state in enumerate(hidden_states): # apply the language model head to the hidden states logits = model.lm_head(hidden_state) # decode the logits to get the predicted token ids predicted_token_ids = logits.argmax(-1) # convert the token ids to tokens predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0], skip_special_tokens=False) predicted_tokens = cleanup_tokens(predicted_tokens) # append the predicted tokens to the list for later logitlens.append(predicted_tokens) print(f&quot;Layer {i}: {predicted_tokens}&quot;) Layer 0: [&apos;-&apos;, &apos; S&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos;-&apos;, &apos; S&apos;, &apos;-&apos;] Layer 1: [&apos;ed&apos;, &apos;oret&apos;, &apos;est&apos;, &apos;ies&apos;, &apos;es&apos;, &apos;uit&apos;, &apos; the&apos;, &apos; same&apos;, &apos; double&apos;] Layer 2: [&apos;import&apos;, &apos;oret&apos;, &apos;est&apos;, &apos;ies&apos;, &apos;es&apos;, &apos;uits&apos;, &apos; time&apos;, &apos; same&apos;, &apos; part&apos;] Layer 3: [&apos;import&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;es&apos;, &apos; all&apos;, &apos; entire&apos;, &apos; man&apos;] Layer 4: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;worked&apos;, &apos; entire&apos;, &apos; man&apos;] Layer 5: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;worked&apos;, &apos; entire&apos;, &apos; man&apos;] Layer 6: [&apos; realise&apos;, &apos;orem&apos;, &apos;est&apos;, &apos;arf&apos;, &apos;es&apos;, &apos;uit&apos;, &apos;kill&apos;, &apos;ses&apos;, &apos; man&apos;] Layer 7: [&apos;iveness&apos;, &apos;orem&apos;, &apos;est&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; all&apos;, &apos; entire&apos;, &apos; brown&apos;] Layer 8: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos;ind&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 9: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 10: [&apos;iveness&apos;, &apos;orem&apos;, &apos;ness&apos;, &apos; ph&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 11: [&apos;iveness&apos;, &apos;orem&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos;ers&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 12: [&apos;iveness&apos;, &apos;oret&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 13: [&apos;ality&apos;, &apos;oret&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 14: [&apos;ality&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; ph&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 15: [&apos;iveness&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; obstacles&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 16: [&apos;import&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; into&apos;, &apos; lazy&apos;, &apos; entire&apos;, &apos; poor&apos;] Layer 17: [&apos;import&apos;, &apos;mes&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; poor&apos;] Layer 18: [&apos;import&apos;, &apos; first&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 19: [&apos; example&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;Ċ&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 20: [&apos;ĊĊ&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;s&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 21: [&apos;ing&apos;, &apos; first&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos;es&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 22: [&apos;Ċ&apos;, &apos; first&apos;, &apos; brown&apos;, &apos;Ċ&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 23: [&apos;Ċ&apos;, &apos;Ċ&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; J&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] Layer 24: [&apos;Ċ&apos;, &apos;ory&apos;, &apos; brown&apos;, &apos; fox&apos;, &apos; jumps&apos;, &apos; over&apos;, &apos; the&apos;, &apos; lazy&apos;, &apos; dog&apos;] As you observe, the predictions refine layer-by-layer, reflecting the model’s gradual understanding of the input. We can visualize the predictions with a heatmap: # create a heatmap that has a row for each list in the logitlens list import matplotlib.pyplot as plt import seaborn as sns import numpy as np sns.set_theme(style=&quot;white&quot;) # just for the bkg color intensities = np.ones((len(hidden_states), len(original_input_tokens))) # Create heatmap plt.figure(figsize=(20, 10)) ax = sns.heatmap(intensities[::2], annot=cleanup_tokens(logitlens)[::2], fmt=&apos;&apos;, cmap=&apos;Greys&apos;, xticklabels=original_input_tokens, yticklabels=list(range(len(logitlens)))[::2], cbar=False ).invert_yaxis() Right now, our heatmap just displays the model’s top predictions (using argmax), which is fine but a bit flat. Let’s make it more interesting by incorporating model certainty into the visualization. A good way to quantify the model’s certainity about its output is looking at the entropy of the output distribution. Let’s replace the background color of each cell with the entropy of the model when generating that token. We’ll calculate the entropy of the output distribution, using it to color the background: # aux function to compute the entropy from logits def entropy_from_logits(logits): probs = torch.nn.functional.softmax(logits, dim=-1).clamp(1e-8, 1) #avoid nans return -torch.sum(probs * torch.log(probs), dim=-1).squeeze() Now we can run the same code as before, this time we’ll also compute and store the entropies. logitlens = [] entropies = [] for i, hidden_state in enumerate(hidden_states): # apply the language model head to the hidden states logits = model.lm_head(hidden_state) # get the entropy of the logits entropy = entropy_from_logits(logits).float().cpu().detach().numpy() # decode the logits to get the predicted token ids predicted_token_ids = logits.argmax(-1) # convert the token ids to tokens predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0], skip_special_tokens=False) predicted_tokens = cleanup_tokens(predicted_tokens) # append the predicted tokens to the list logitlens.append(predicted_tokens) entropies.append(entropy) print(f&quot;Layer {i}: {predicted_tokens}&quot;) Let’s now create a plot where each cell is colored based on the entropy. # Create figure and axis plt.figure(figsize=(20, 10)) # Create heatmap ax = sns.heatmap(np.stack(entropies)[::2], annot=logitlens[::2], fmt=&apos;&apos;, cmap=&apos;YlGnBu&apos;, xticklabels=original_input_tokens, yticklabels=list(range(len(logitlens)))[::2], ).invert_yaxis() Hope you liked this! If you have any suggestions/questios, feel free to drop me a message/email or visit my page or my twitter @devoto_alessio.</summary></entry></feed>